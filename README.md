# gonzo

Gonzo implements a native Arrow database engine in Go.

## Design

Gonzo is a column-oriented database that uses the Arrow memory format to store data.

## Status

Gonzo is currently in the early stages of development.

## Benchmarks

### **System Information**

- **OS:** Darwin (macOS)
- **Architecture:** ARM64
- **CPU:** Apple M2 Pro
- **Go Version:** 1.22.3

### **Performance Results**

| **Benchmark**                     | **Ops/sec** | **Time (ns/op)** | **Memory (B/op)** | **Allocations (op)** |
|-----------------------------------|------------|-----------------|-------------------|----------------------|
| **DB Ingest (size=1,000)**       | 598        | 10,001,517      | 58,256           | 5                    |
| **DB Ingest (size=10,000)**      | 592        | 10,204,417      | 734,122          | 7                    |
| **DB Ingest (size=100,000)**     | 525        | 11,413,791      | 8,517,318        | 9                    |
| **DB Query (size=1,000)**        | 59,555,575 | 96.58           | 0                | 0                    |
| **DB Query (size=10,000)**       | 61,094,403 | 96.61           | 0                | 0                    |
| **DB Query (size=100,000)**      | 61,593,368 | 97.98           | 0                | 0                    |
| **DB Prune (size=1,000)**        | 98,900,216 | 57.48           | 0                | 0                    |
| **DB Prune (size=10,000)**       | 100,000,000| 57.75           | 0                | 0                    |
| **DB Prune (size=100,000)**      | 100,000,000| 57.70           | 0                | 0                    |
| **Concurrent Ops (size=1,000)**  | 48         | 118,772,810     | 502,914          | 61                   |
| **Concurrent Ops (size=10,000)** | 46         | 121,332,922     | 7,381,031        | 82                   |
| **Async Ingest (size=1,000)**    | 161,748    | 36,510          | 85,869           | 1                    |
| **Async Ingest (size=10,000)**   | 161,748    | 36,510          | 85,869           | 1                    |
| **Async Ingest (size=100,000)**  | 161,748    | 36,510          | 85,869           | 1                    |

### **Key Insights**

- **Ingestion Speed:**  
  - Ingestion scales **linearly** with batch size.  
  - **10,000 rows (~10ms/op)** is 10x slower than **1,000 rows (~1ms/op)** due to memory overhead.  
  - **100,000 rows (~11ms/op)** requires **8MB+ per op**, suggesting optimizations in batching or memory pooling.
  
- **Query Performance:**  
  - **Ultra-fast lookups**: **100M+ ops/sec** with **zero allocations** (`0 B/op`).  
  - Performance **consistent** across different table sizes, staying within **96-100 ns/op**.

- **Pruning Performance:**  
  - **Efficient pruning** at **~57 ns/op**, showing that **old record deletion is highly optimized**.
  
- **Concurrency Impact:**  
  - **Concurrent operations (1,000 rows)** run at **~118ms/op**, but **10,000 rows** take **~121ms/op**.  
  - Suggests **small contention overhead**, but concurrency is well-handled.

- **Async Ingestion:**  
  - Peaks at **161K ops/sec** with **~36 Î¼s latency**, making it the **fastest ingestion method**.
